# -*- coding: utf-8 -*-
"""S_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t-j3h7LdlMCHZ-vJRp9bCI_AOLj6Cct5
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
import sklearn
# %matplotlib inline
# %config InlineBackend.figure_format='retina'
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

df = pd.read_csv('/content/drive/MyDrive/Research/weather_five_minutes.csv')

df.drop(['--Timestamp---',' UV '],axis=1,inplace=True)

train_size = int(len(df) * 0.8)
test_size = len(df) - train_size
train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]
print(len(train), len(test))

from sklearn.preprocessing import MinMaxScaler
f_columns = ['Temp', 'Chill', 'Humid', 'Dewpt',' Wind ','HiWind','Rain ','Barom',' ET  ']
f_transformer = MinMaxScaler()
f_transformer = f_transformer.fit(train[f_columns].to_numpy())
train.loc[:, f_columns] = f_transformer.transform(
  train[f_columns].to_numpy()
)
test.loc[:, f_columns] = f_transformer.transform(
  test[f_columns].to_numpy()
)

solar_transformer = MinMaxScaler()
solar = solar_transformer.fit(train[['Solar']])
train['Solar'] = solar.transform(train[['Solar']])
test['Solar'] = solar.transform(test[['Solar']])

def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X.iloc[i:(i + time_steps)].values
        Xs.append(v)
        ys.append(y.iloc[i + time_steps])
    return np.array(Xs), np.array(ys)

time_steps = 1
# reshape to [samples, time_steps, n_features]
X_train, y_train = create_dataset(train, train.Solar, time_steps)
X_test, y_test = create_dataset(test, test.Solar, time_steps)
print(X_train.shape, y_train.shape)

from keras.layers import LSTM

model1 = keras.models.Sequential()
model1.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]),return_sequences=True))
model1.add(tf.keras.layers.LeakyReLU(alpha=0.5))
model1.add(LSTM(64, return_sequences=True))
model1.add(tf.keras.layers.LeakyReLU(alpha=0.5))
model1.add(tf.keras.layers.Dropout(0.5))
model1.add(LSTM(32, return_sequences=True))
model1.add(tf.keras.layers.LeakyReLU(alpha=0.5))
model1.add(tf.keras.layers.Dropout(0.5))
model1.add((LSTM(16,return_sequences=False)))
model1.add(tf.keras.layers.Dropout(0.5))
model1.add(tf.keras.layers.Dense(1))

optimizer = keras.optimizers.Adam()
model1.compile(loss='mean_squared_error', optimizer=optimizer)

history1= model1.fit(
    X_train, y_train,
    epochs=30,
    batch_size=256,
    validation_split=0.1,
    shuffle=False
)

plt.plot(history1.history['loss'],marker='o',label='train')
plt.plot(history1.history['val_loss'],marker='o',label='validation')
plt.title('LSTM_0')
plt.ylabel('loss ')
plt.xlabel('epochs')
plt.legend()

y_pred1=model1.predict(X_test)

from sklearn.metrics import mean_squared_error
print(mean_squared_error(y_test,y_pred1))
print(np.sqrt(mean_squared_error(y_test,y_pred1)))
from sklearn.metrics import mean_absolute_error
print(mean_absolute_error(y_test,y_pred1))

def percentage_error(actual, predicted):
    res = np.empty(actual.shape)
    for j in range(actual.shape[0]):
        if actual[j] != 0:
            res[j] = (actual[j] - predicted[j]) / actual[j]
        else:
            res[j] = predicted[j] / np.mean(actual)
    return res

def mean_absolute_percentage_error(y_test, y_pred): 
    return np.mean(np.abs(percentage_error(np.asarray(y_test), np.asarray(y_pred)))) * 100

mean_absolute_percentage_error(y_test, y_pred1)

plt.plot(y_test[0:1000],color='k',label="Actual ")
plt.plot(y_pred1[0:1000],color="r",label="Forecast ")

plt.xlabel('Number of Observation')
plt.ylabel('Solar Energy(W/mÂ²)')
plt.title('Solar energy prediction using LSTM_0',fontsize=15)
plt.legend()
plt.show()